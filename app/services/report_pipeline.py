"""
æŠ¥å‘Šç”Ÿæˆç®¡çº¿

ä¸²è”æ‰€æœ‰æ¨¡å—ï¼Œç”Ÿæˆå®Œæ•´çš„é‚®ä»¶æŠ¥å‘Š
"""
import re
from datetime import date, timedelta
from typing import Optional, Dict, Any, List, Tuple

from google.oauth2.credentials import Credentials

from app.core.logging import get_logger
from app.integrations.gmail.auth import SkillGmailAuth, AuthError
from app.integrations.gmail.fetch import SkillGmailFetch
from app.integrations.gmail.normalize import SkillEmailNormalize
from app.services.thread_merge import SkillThreadMerge
from app.services.importance import SkillImportanceHeuristics
from app.integrations.openai.prompts import SkillPromptCompose
from app.integrations.openai.summarize import SkillGptSummarize, GptError
from app.integrations.anthropic.summarize import SkillClaudeSummarize, ClaudeError
from app.core.config import config as app_config
from app.db.report_store import SkillReportStore, ReportData

logger = get_logger(__name__)


class ReportPipelineError(Exception):
    """æŠ¥å‘Šç”Ÿæˆç®¡çº¿é”™è¯¯"""
    pass


class ReportPipeline:
    """
    æŠ¥å‘Šç”Ÿæˆç®¡çº¿

    å®Œæ•´çš„æŠ¥å‘Šç”Ÿæˆæµç¨‹
    """

    @staticmethod
    def generate_report_for_date(
        report_date: date,
        credentials: Optional[Credentials] = None,
        last_n_hours: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆæŒ‡å®šæ—¥æœŸçš„é‚®ä»¶æŠ¥å‘Š

        Args:
            report_date: æŠ¥å‘Šæ—¥æœŸ
            credentials: Google å‡­æ®ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨åŠ è½½
            last_n_hours: æ‹‰å–æœ€è¿‘ N å°æ—¶çš„é‚®ä»¶ï¼Œå¦‚æœæŒ‡å®šåˆ™å¿½ç•¥ date èŒƒå›´

        Returns:
            ç”Ÿæˆçš„æŠ¥å‘Šå­—å…¸ï¼ŒåŒ…å« report_id å’Œ summary

        Raises:
            AuthError: è®¤è¯å¤±è´¥
            ReportPipelineError: ç®¡çº¿æ‰§è¡Œå¤±è´¥
        """
        logger.info("=" * 60)
        logger.info(f"å¼€å§‹ç”ŸæˆæŠ¥å‘Š: {report_date}")
        logger.info("=" * 60)

        try:
            # Step 1: åŠ è½½å‡­æ®
            logger.info("æ­¥éª¤ 1/7: åŠ è½½ Gmail å‡­æ®")
            if credentials is None:
                credentials = SkillGmailAuth.load_credentials()
                if credentials is None:
                    raise AuthError("æœªæ‰¾åˆ°æœ‰æ•ˆå‡­æ®ï¼Œè¯·å…ˆè¿›è¡Œæˆæƒ", needs_reauth=True)
            logger.info("âœ“ å‡­æ®åŠ è½½æˆåŠŸ")

            # Step 2: æ‹‰å–é‚®ä»¶
            logger.info("æ­¥éª¤ 2/7: æ‹‰å–é‚®ä»¶")
            gmail_max_results = getattr(app_config, "GMAIL_MAX_RESULTS", 100)
            if last_n_hours:
                # æœ€è¿‘ N å°æ—¶
                messages = SkillGmailFetch.fetch_messages(
                    credentials=credentials,
                    last_n_hours=last_n_hours,
                    max_results=gmail_max_results
                )
                logger.info(f"âœ“ æ‹‰å–äº†æœ€è¿‘ {last_n_hours} å°æ—¶çš„ {len(messages)} å°é‚®ä»¶")
            else:
                # æŒ‡å®šæ—¥æœŸï¼ˆå½“å¤© 00:00 åˆ° 23:59ï¼‰
                messages = SkillGmailFetch.fetch_messages(
                    credentials=credentials,
                    date_from=report_date,
                    date_to=report_date,
                    max_results=gmail_max_results
                )
                logger.info(f"âœ“ æ‹‰å–äº† {len(messages)} å°é‚®ä»¶")

            # æ£€æŸ¥æ˜¯å¦æœ‰é‚®ä»¶
            if not messages:
                logger.info("æ²¡æœ‰é‚®ä»¶ï¼Œç”Ÿæˆç©ºæŠ¥å‘Š")
                return ReportPipeline._generate_empty_report(report_date)

            # Step 3: å½’ä¸€åŒ–é‚®ä»¶
            logger.info("æ­¥éª¤ 3/7: å½’ä¸€åŒ–é‚®ä»¶")
            normalized_emails = []
            for msg in messages:
                normalized = SkillEmailNormalize.normalize(msg)
                normalized_emails.append(normalized)
            logger.info(f"âœ“ å½’ä¸€åŒ–äº† {len(normalized_emails)} å°é‚®ä»¶")

            # Step 4: åˆå¹¶çº¿ç¨‹
            logger.info("æ­¥éª¤ 4/7: åˆå¹¶çº¿ç¨‹")
            threads = SkillThreadMerge.merge_threads(normalized_emails)
            logger.info(f"âœ“ åˆå¹¶ä¸º {len(threads)} ä¸ªçº¿ç¨‹")

            truncated_threads = sum(1 for t in threads if getattr(t, "is_truncated", False))
            if truncated_threads:
                logger.warning(f"æœ‰ {truncated_threads}/{len(threads)} ä¸ªçº¿ç¨‹å› å†…å®¹è¿‡é•¿è¢«æˆªæ–­ï¼ˆThreadMerge.MAX_COMBINED_LENGTH é™åˆ¶ï¼‰")

            # Step 5: é‡è¦æ€§è¯„åˆ†
            logger.info("æ­¥éª¤ 5/7: é‡è¦æ€§è¯„åˆ†")
            scorer = SkillImportanceHeuristics()
            scored_threads = scorer.prioritize_threads(threads)
            logger.info(f"âœ“ å®Œæˆè¯„åˆ†ï¼Œæœ€é«˜åˆ†: {scored_threads[0][1]:.1f}" if scored_threads else "âœ“ å®Œæˆè¯„åˆ†")

            # Step 6: ç”Ÿæˆ Prompt å¹¶è°ƒç”¨ AI
            ai_provider = app_config.AI_PROVIDER
            logger.info(f"æ­¥éª¤ 6/7: è°ƒç”¨ {ai_provider.upper()} ç”ŸæˆæŠ¥å‘Š")

            try:
                summary = ReportPipeline._generate_ai_summary(
                    scored_threads=scored_threads,
                    report_date=report_date,
                    ai_provider=ai_provider
                )
                logger.info(f"âœ“ {ai_provider.upper()} æŠ¥å‘Šç”ŸæˆæˆåŠŸ")

            except (GptError, ClaudeError) as e:
                logger.error(f"AI è°ƒç”¨å¤±è´¥: {e}")
                # ç”Ÿæˆé™çº§æŠ¥å‘Šï¼ˆä¸ä½¿ç”¨ AIï¼‰
                summary = ReportPipeline._generate_fallback_summary(scored_threads)
                logger.info("âœ“ ä½¿ç”¨é™çº§æŠ¥å‘Šï¼ˆæœªè°ƒç”¨ AIï¼‰")

            # Step 7: ä¿å­˜æŠ¥å‘Š
            logger.info("æ­¥éª¤ 7/7: ä¿å­˜æŠ¥å‘Šåˆ°æ•°æ®åº“")

            # æ„å»ºé‚®ä»¶å¼•ç”¨
            email_refs = []
            max_ref_threads = getattr(app_config, "REPORT_MAX_REF_THREADS", 20)
            ref_threads = scored_threads if (max_ref_threads is None or max_ref_threads <= 0) else scored_threads[:max_ref_threads]
            if max_ref_threads is not None and max_ref_threads > 0 and len(scored_threads) > max_ref_threads:
                logger.warning(f"é‚®ä»¶å¼•ç”¨ä»…ä¿å­˜å‰ {max_ref_threads} ä¸ªçº¿ç¨‹ï¼ˆå…± {len(scored_threads)} ä¸ªï¼‰ã€‚å¯é€šè¿‡ REPORT_MAX_REF_THREADS è°ƒæ•´ã€‚")

            for thread, score in ref_threads:
                for msg in thread.messages:
                    email_refs.append({
                        'message_id': msg.message_id,
                        'thread_id': msg.thread_id,
                        'subject': msg.subject,
                        'from_addr': msg.from_addr,
                        'to_addr': msg.to_addr,
                        'date': msg.date,
                        'snippet': msg.snippet,
                        'gmail_url': f"https://mail.google.com/mail/u/0/#inbox/{msg.message_id}"
                    })

            # åˆ›å»ºæŠ¥å‘Šæ•°æ®
            report_data = ReportData(
                date=report_date,
                summary=summary,
                email_refs=email_refs
            )

            # ä¿å­˜
            report_id = SkillReportStore.save_report(report_data)
            logger.info(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜ï¼ŒID: {report_id}")

            logger.info("=" * 60)
            logger.info(f"æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼æŠ¥å‘Š ID: {report_id}")
            logger.info("=" * 60)

            return {
                'report_id': report_id,
                'date': report_date.isoformat(),
                'summary': summary,
                'email_count': len(messages),
                'thread_count': len(threads),
                'reference_count': len(email_refs)
            }

        except AuthError:
            # é‡æ–°æŠ›å‡ºè®¤è¯é”™è¯¯
            raise

        except Exception as e:
            logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            raise ReportPipelineError(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}") from e

    @staticmethod
    def _generate_empty_report(report_date: date) -> Dict[str, Any]:
        """
        ç”Ÿæˆç©ºæŠ¥å‘Š

        Args:
            report_date: æŠ¥å‘Šæ—¥æœŸ

        Returns:
            æŠ¥å‘Šå­—å…¸
        """
        summary = {
            'highlights': ['ä»Šæ—¥æ— æ–°é‚®ä»¶'],
            'todos': [],
            'categories': {
                'action_required': [],
                'important': [],
                'billing': [],
                'social': [],
                'other': []
            }
        }

        report_data = ReportData(
            date=report_date,
            summary=summary,
            email_refs=[]
        )

        report_id = SkillReportStore.save_report(report_data)
        logger.info(f"âœ“ ç©ºæŠ¥å‘Šå·²ä¿å­˜ï¼ŒID: {report_id}")

        return {
            'report_id': report_id,
            'date': report_date.isoformat(),
            'summary': summary,
            'email_count': 0,
            'thread_count': 0,
            'reference_count': 0
        }

    @staticmethod
    def _generate_fallback_summary(scored_threads) -> Dict[str, Any]:
        """
        ç”Ÿæˆé™çº§æ‘˜è¦ï¼ˆä¸ä½¿ç”¨ AIï¼‰

        ä¸ AI æŠ¥å‘Šæ ¼å¼ä¸€è‡´ï¼šé‡è¦ï¼ˆscore>=20ï¼‰ä»… 3 å­—æ®µï¼Œéé‡è¦ä»…å‘ä»¶äºº+ä¸€å¥è¯æ‘˜è¦ã€‚
        ä¿ç•™ ## âš¡ ä»Šæ—¥é‡ç‚¹ ä¸ ## âœ… è¡ŒåŠ¨æ¸…å• ä»¥å…¼å®¹è§£æå™¨ã€‚

        Args:
            scored_threads: è¯„åˆ†åçš„çº¿ç¨‹åˆ—è¡¨

        Returns:
            Markdown æ ¼å¼çš„ç®€å•æŠ¥å‘Š
        """
        # æŒ‰ score>=20 åˆ†ä¸ºé‡è¦ / éé‡è¦
        important = [(t, s) for t, s in scored_threads if s >= 20]
        non_important = [(t, s) for t, s in scored_threads if s < 20]

        total_emails = sum(t.total_messages for t, _ in scored_threads)
        report_parts = [
            "*æ­¤æŠ¥å‘Šæœªä½¿ç”¨ AI ç”Ÿæˆ*",
            ""
        ]

        # ## ğŸ“§ é‡è¦é‚®ä»¶ï¼šä»… å‘ä»¶äºº / æ—¶é—´ / å†…å®¹æ‘˜è¦
        report_parts.append("## ğŸ“§ é‡è¦é‚®ä»¶")
        report_parts.append("")
        if important:
            for thread, _ in important[:20]:
                sender_display = "æœªçŸ¥å‘ä»¶äºº"
                time_str = "æœªçŸ¥"
                snippet = ""
                if thread.messages:
                    msg = thread.messages[0]
                    sender_display = msg.sender_name or msg.from_addr or "æœªçŸ¥"
                    time_str = msg.date.strftime("%Y-%m-%d %H:%M") if msg.date else "æœªçŸ¥"
                    snippet = (msg.snippet or "")[:200]
                    if len(msg.snippet or "") > 200:
                        snippet += "..."
                report_parts.append(f"**{thread.subject}**")
                report_parts.append(f"- **å‘ä»¶äºº**: {sender_display}")
                report_parts.append(f"- **æ—¶é—´**: {time_str}")
                report_parts.append(f"- **å†…å®¹æ‘˜è¦**: {snippet or 'ï¼ˆæ— æ‘˜è¦ï¼‰'}")
                report_parts.append("")
        else:
            report_parts.append("ï¼ˆæ— ï¼‰")
            report_parts.append("")

        # ## ğŸ“‹ éé‡è¦é‚®ä»¶ï¼šå‘ä»¶äºº + ä¸€å¥è¯æ‘˜è¦
        report_parts.append("## ğŸ“‹ éé‡è¦é‚®ä»¶")
        report_parts.append("")
        if non_important:
            for thread, _ in non_important[:30]:
                sender_display = "æœªçŸ¥"
                one_line = ""
                if thread.messages:
                    msg = thread.messages[0]
                    sender_display = msg.sender_name or msg.from_addr or "æœªçŸ¥"
                    one_line = (msg.snippet or thread.subject or "")[:80]
                    if len(msg.snippet or thread.subject or "") > 80:
                        one_line += "..."
                report_parts.append(f"**{thread.subject}** â€” **å‘ä»¶äºº**: {sender_display}ã€‚{one_line}")
            report_parts.append("")
        else:
            report_parts.append("ï¼ˆæ— ï¼‰")
            report_parts.append("")

        # ## âš¡ ä»Šæ—¥é‡ç‚¹
        report_parts.append("## âš¡ ä»Šæ—¥é‡ç‚¹")
        report_parts.append("")
        report_parts.append(f"- å…±æ”¶åˆ° {len(scored_threads)} ä¸ªé‚®ä»¶çº¿ç¨‹ï¼Œ{total_emails} å°é‚®ä»¶")
        if important:
            report_parts.append("- è¯·ä¼˜å…ˆæŸ¥çœ‹ã€Œé‡è¦é‚®ä»¶ã€ç« èŠ‚")
        report_parts.append("")

        # ## âœ… è¡ŒåŠ¨æ¸…å•
        report_parts.append("## âœ… è¡ŒåŠ¨æ¸…å•")
        report_parts.append("")
        if important:
            report_parts.append("- [ ] æŸ¥çœ‹å¹¶å¤„ç†é‡è¦é‚®ä»¶")
        report_parts.append("- [ ] æµè§ˆéé‡è¦é‚®ä»¶æ‘˜è¦")
        report_parts.append("")

        markdown_content = "\n".join(report_parts)

        return {
            'format': 'markdown',
            'full_content': markdown_content,
            'highlights': [f"å…±æ”¶åˆ° {len(scored_threads)} ä¸ªé‚®ä»¶çº¿ç¨‹"] + (["è¯·ä¼˜å…ˆæŸ¥çœ‹é‡è¦é‚®ä»¶"] if important else []),
            'todos': ["æŸ¥çœ‹å¹¶å¤„ç†é‡è¦é‚®ä»¶"] if important else ["æŸ¥çœ‹ä»Šæ—¥é‚®ä»¶"],
            'sections': {}
        }

    @staticmethod
    def _generate_ai_summary(
        scored_threads,
        report_date: date,
        ai_provider: str,
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆ AI æŠ¥å‘Šæ‘˜è¦ï¼ˆæ”¯æŒçº¿ç¨‹åˆ†æ‰¹æ€»ç»“ï¼Œé¿å…åªæ€»ç»“å‰ N ä¸ªçº¿ç¨‹ï¼‰
        """
        max_threads_per_prompt = getattr(app_config, "PROMPT_MAX_THREADS", 50)
        if max_threads_per_prompt is None or max_threads_per_prompt <= 0:
            max_threads_per_prompt = len(scored_threads)

        # åˆ†æ‰¹
        batches = [
            scored_threads[i:i + max_threads_per_prompt]
            for i in range(0, len(scored_threads), max_threads_per_prompt)
        ]

        def call_ai(system_prompt: str, user_prompt: str) -> Dict[str, Any]:
            if ai_provider == "claude":
                ai_client = SkillClaudeSummarize()
                return ai_client.summarize(system_prompt, user_prompt)
            ai_client = SkillGptSummarize()
            return ai_client.summarize(system_prompt, user_prompt)

        # å…ˆå¯¹æ¯ä¸ª batch ç”Ÿæˆä¸€ä»½æŠ¥å‘Šï¼ˆæˆ‘ä»¬åªå–å…¶ä¸­â€œé‡è¦/éé‡è¦â€ä¸¤èŠ‚ï¼Œåç»­ç»Ÿä¸€æ±‡æ€»é‡ç‚¹ä¸å¾…åŠï¼‰
        batch_reports: list[Dict[str, Any]] = []
        for idx, batch in enumerate(batches, 1):
            logger.info(f"åˆ†æ‰¹æ€»ç»“çº¿ç¨‹ï¼š{idx}/{len(batches)}ï¼ˆæœ¬æ‰¹ {len(batch)} ä¸ªçº¿ç¨‹ï¼‰")
            parsed = ReportPipeline._generate_batch_with_coverage_check(
                batch=batch,
                report_date=report_date,
                call_ai=call_ai,
            )
            batch_reports.append(parsed)

        # æ±‡æ€»â€œé‡è¦é‚®ä»¶/éé‡è¦é‚®ä»¶â€ä¸¤èŠ‚å†…å®¹
        def pick_section(sections: Dict[str, str], preferred_titles: list[str]) -> str:
            for t in preferred_titles:
                v = sections.get(t)
                if v and v.strip():
                    return v.strip()
            return ""

        important_chunks = []
        non_important_chunks = []
        for br in batch_reports:
            sections = br.get("sections", {}) or {}
            important = pick_section(sections, ["ğŸ“§ é‡è¦é‚®ä»¶", "é‡è¦é‚®ä»¶"])
            non_important = pick_section(sections, ["ğŸ“‹ éé‡è¦é‚®ä»¶", "éé‡è¦é‚®ä»¶"])
            if important:
                important_chunks.append(important)
            if non_important:
                non_important_chunks.append(non_important)

        important_section = "\n\n".join(important_chunks).strip() or "ï¼ˆæ— ï¼‰"
        non_important_section = "\n\n".join(non_important_chunks).strip() or "ï¼ˆæ— ï¼‰"

        # åŸºäºæ±‡æ€»åçš„â€œé‡è¦/éé‡è¦â€ç« èŠ‚ï¼Œå†ç”Ÿæˆä¸€æ¬¡â€œä»Šæ—¥é‡ç‚¹/è¡ŒåŠ¨æ¸…å•â€ï¼ˆå†…å®¹æ›´çŸ­ã€æ›´ä¸æ˜“è¶…é™ï¼‰
        finalize_system = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‚®ä»¶åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡ Markdown è¾“å‡ºæŒ‡å®šç« èŠ‚ï¼Œå†…å®¹è¦å…·ä½“ã€å¯æ‰§è¡Œã€‚"
        finalize_user = "\n".join([
            "è¯·æ ¹æ®ä»¥ä¸‹å·²æ•´ç†çš„é‚®ä»¶æ‘˜è¦ï¼Œç”Ÿæˆ**ä»…åŒ…å«**è¿™ä¸¤ä¸ªç« èŠ‚ï¼ˆé¡ºåºä¸å¯å˜ï¼‰ï¼š",
            "",
            "## âš¡ ä»Šæ—¥é‡ç‚¹",
            "",
            "3-5æ¡æœ€å…³é”®çš„å‘ç°æˆ–å¾…åŠäº‹é¡¹ï¼ˆå°½é‡ä»é‡è¦é‚®ä»¶ä¸­æç‚¼ï¼‰ã€‚",
            "",
            "## âœ… è¡ŒåŠ¨æ¸…å•",
            "",
            "å…·ä½“çš„å¾…åŠäº‹é¡¹åˆ—è¡¨ï¼ˆç”¨ Markdown åˆ—è¡¨ï¼Œå¯å¸¦å¤é€‰æ¡†ï¼‰ã€‚",
            "",
            "---",
            "",
            "ä»¥ä¸‹æ˜¯æ•´ç†å¥½çš„æ‘˜è¦ï¼š",
            "",
            "## ğŸ“§ é‡è¦é‚®ä»¶",
            important_section,
            "",
            "## ğŸ“‹ éé‡è¦é‚®ä»¶",
            non_important_section,
        ])

        finalize_resp = call_ai(finalize_system, finalize_user)
        if finalize_resp.get("format") == "markdown":
            finalize_parsed = ReportPipeline._parse_markdown_report(finalize_resp["content"])
        else:
            finalize_parsed = ReportPipeline._fix_report_structure(finalize_resp)

        final_sections = finalize_parsed.get("sections", {}) or {}
        highlights_body = pick_section(final_sections, ["âš¡ ä»Šæ—¥é‡ç‚¹", "ä»Šæ—¥é‡ç‚¹", "é‡ç‚¹"]) or "ï¼ˆæ— ï¼‰"
        todos_body = pick_section(final_sections, ["âœ… è¡ŒåŠ¨æ¸…å•", "è¡ŒåŠ¨æ¸…å•", "å¾…åŠäº‹é¡¹", "å¾…åŠ"]) or "ï¼ˆæ— ï¼‰"

        full_markdown = "\n".join([
            "## ğŸ“§ é‡è¦é‚®ä»¶",
            "",
            important_section,
            "",
            "## ğŸ“‹ éé‡è¦é‚®ä»¶",
            "",
            non_important_section,
            "",
            "## âš¡ ä»Šæ—¥é‡ç‚¹",
            "",
            highlights_body,
            "",
            "## âœ… è¡ŒåŠ¨æ¸…å•",
            "",
            todos_body,
        ]).strip()

        return ReportPipeline._parse_markdown_report(full_markdown)

    @staticmethod
    def _extract_thread_tags(text: str) -> set:
        """ä»ç« èŠ‚æ–‡æœ¬ä¸­æå– [Txx] æ ‡ç­¾é›†åˆï¼Œå¦‚ {'T01', 'T02'}"""
        if not text:
            return set()
        tags = re.findall(r'\[(T\d+)\]', text, re.IGNORECASE)
        return {t.upper() for t in tags}

    @staticmethod
    def _validate_batch_coverage(parsed: Dict[str, Any], batch_size: int) -> Tuple[bool, set]:
        """
        æ ¡éªŒé‡è¦+éé‡è¦ç« èŠ‚ä¸­ [Txx] æ˜¯å¦è¦†ç›–å…¨éƒ¨çº¿ç¨‹ã€‚
        Returns: (æ˜¯å¦é€šè¿‡, ç¼ºå¤±çš„æ ‡ç­¾é›†åˆ)
        """
        def pick_section(sections: Dict[str, str], preferred_titles: list[str]) -> str:
            for t in preferred_titles:
                v = sections.get(t)
                if v and v.strip():
                    return v.strip()
            return ""

        sections = parsed.get("sections", {}) or {}
        important = pick_section(sections, ["ğŸ“§ é‡è¦é‚®ä»¶", "é‡è¦é‚®ä»¶"])
        non_important = pick_section(sections, ["ğŸ“‹ éé‡è¦é‚®ä»¶", "éé‡è¦é‚®ä»¶"])
        found = ReportPipeline._extract_thread_tags(important) | ReportPipeline._extract_thread_tags(non_important)
        expected = {f"T{i:02d}" for i in range(1, batch_size + 1)}
        missing = expected - found
        return (len(missing) == 0, missing)

    @staticmethod
    def _generate_batch_with_coverage_check(
        batch: list,
        report_date: date,
        call_ai,
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå•ä¸ª batch çš„æŠ¥å‘Šï¼Œè‹¥è¦†ç›–ç‡ä¸è¶³åˆ™å‘èµ·è¡¥é½é‡è¯•ã€‚
        """
        system_prompt, user_prompt = SkillPromptCompose.compose(batch, report_date)
        ai_response = call_ai(system_prompt, user_prompt)

        if ai_response.get("format") == "markdown":
            parsed = ReportPipeline._parse_markdown_report(ai_response["content"])
        else:
            parsed = ai_response
            parsed = ReportPipeline._fix_report_structure(parsed)

        ok, missing = ReportPipeline._validate_batch_coverage(parsed, len(batch))
        if ok:
            return parsed

        logger.warning(f"æœ¬æ‰¹ {len(batch)} ä¸ªçº¿ç¨‹ï¼Œç¼ºå¤± {len(missing)} æ¡: {sorted(missing)}ï¼Œå‘èµ·è¡¥é½é‡è¯•")
        supplement = ReportPipeline._generate_supplement_for_missing(
            batch=batch,
            report_date=report_date,
            missing_tags=missing,
            call_ai=call_ai,
        )
        if supplement:
            parsed = ReportPipeline._merge_supplement_into_parsed(parsed, supplement)
            ok2, missing2 = ReportPipeline._validate_batch_coverage(parsed, len(batch))
            if not ok2:
                logger.warning(f"è¡¥é½åä»ç¼ºå¤± {len(missing2)} æ¡: {sorted(missing2)}")
        return parsed

    @staticmethod
    def _generate_supplement_for_missing(
        batch: list,
        report_date: date,
        missing_tags: set,
        call_ai,
    ) -> Optional[Dict[str, str]]:
        """
        ä¸ºç¼ºå¤±çš„ [Txx] ç”Ÿæˆè¡¥é½å†…å®¹ã€‚è¿”å› {'important': str, 'non_important': str} æˆ– Noneã€‚
        """
        # å»ºç«‹ tag -> (thread, score) æ˜ å°„
        tag_to_item = {}
        for i, (thread, score) in enumerate(batch, 1):
            tag = f"T{i:02d}"
            tag_to_item[tag] = (thread, score)

        missing_list = sorted(missing_tags)
        supplement_threads = []
        for tag in missing_list:
            if tag in tag_to_item:
                supplement_threads.append((tag, tag_to_item[tag]))

        if not supplement_threads:
            return None

        # æ„å»ºä»…åŒ…å«ç¼ºå¤±çº¿ç¨‹çš„ prompt
        user_parts = [
            f"ä»¥ä¸‹ {len(supplement_threads)} ä¸ªçº¿ç¨‹åœ¨ä¹‹å‰çš„æŠ¥å‘Šä¸­é—æ¼ï¼Œè¯·**ä»…**ä¸ºå®ƒä»¬è¾“å‡ºæ¡ç›®ï¼Œæ ¼å¼ä¸ä¹‹å‰ç›¸åŒã€‚",
            "",
            "ç¼ºå¤±çš„çº¿ç¨‹åŠå…¶åœ¨è¾“å‡ºä¸­çš„æ ‡ç­¾ï¼š",
            ""
        ]
        for tag, (thread, score) in supplement_threads:
            user_parts.append(f"### [{tag}] çº¿ç¨‹ (é‡è¦æ€§: {score:.1f})")
            user_parts.append(f"ä¸»é¢˜: {thread.subject}")
            user_parts.append(f"é‚®ä»¶æ•°: {thread.total_messages}")
            if thread.participants:
                user_parts.append(f"å‚ä¸è€…: {', '.join(thread.participants[:3])}")
            if thread.has_attachments:
                user_parts.append("ğŸ“ åŒ…å«é™„ä»¶")
            user_parts.append(f"å†…å®¹:\n{thread.combined_text[:2000]}")  # é™åˆ¶é•¿åº¦
            user_parts.append("")

        user_parts.append("è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œåˆ†æ•°>=20 çš„æ”¾å…¥ ## ğŸ“§ é‡è¦é‚®ä»¶ï¼Œ<20 çš„æ”¾å…¥ ## ğŸ“‹ éé‡è¦é‚®ä»¶ï¼š")
        user_parts.append("- é‡è¦: **[Txx] ä¸»é¢˜** + å‘ä»¶äººã€æ—¶é—´ã€å†…å®¹æ‘˜è¦")
        user_parts.append("- éé‡è¦: **[Txx] ä¸»é¢˜** â€” **å‘ä»¶äºº**: xxxã€‚ä¸€å¥è¯æ‘˜è¦ã€‚")

        sup_system = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‚®ä»¶åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼Œæ¯æ¡æ ‡é¢˜å¿…é¡»ä»¥å¯¹åº”çš„ [Txx] å¼€å¤´ã€‚"
        sup_user = "\n".join(user_parts)

        try:
            resp = call_ai(sup_system, sup_user)
            content = resp.get("content", "").strip() if resp.get("format") == "markdown" else ""
            if not content:
                return None
            sup_parsed = ReportPipeline._parse_markdown_report(content)
            sections = sup_parsed.get("sections", {}) or {}
            important = sections.get("ğŸ“§ é‡è¦é‚®ä»¶", "").strip() or sections.get("é‡è¦é‚®ä»¶", "").strip()
            non_important = sections.get("ğŸ“‹ éé‡è¦é‚®ä»¶", "").strip() or sections.get("éé‡è¦é‚®ä»¶", "").strip()
            return {"important": important, "non_important": non_important}
        except Exception as e:
            logger.warning(f"è¡¥é½é‡è¯•å¤±è´¥: {e}")
            return None

    @staticmethod
    def _merge_supplement_into_parsed(parsed: Dict[str, Any], supplement: Dict[str, str]) -> Dict[str, Any]:
        """å°†è¡¥é½å†…å®¹è¿½åŠ åˆ° parsed çš„å¯¹åº”ç« èŠ‚ã€‚"""
        sections = parsed.get("sections", {}) or {}
        imp_key = "ğŸ“§ é‡è¦é‚®ä»¶" if "ğŸ“§ é‡è¦é‚®ä»¶" in sections else "é‡è¦é‚®ä»¶"
        non_key = "ğŸ“‹ éé‡è¦é‚®ä»¶" if "ğŸ“‹ éé‡è¦é‚®ä»¶" in sections else "éé‡è¦é‚®ä»¶"

        imp_cur = sections.get(imp_key, "").strip()
        non_cur = sections.get(non_key, "").strip()
        imp_add = (supplement.get("important") or "").strip()
        non_add = (supplement.get("non_important") or "").strip()

        if imp_add:
            sections[imp_key] = (imp_cur + "\n\n" + imp_add).strip() if imp_cur else imp_add
        if non_add:
            sections[non_key] = (non_cur + "\n\n" + non_add).strip() if non_cur else non_add

        parsed["sections"] = sections
        return parsed

    @staticmethod
    def _parse_markdown_report(markdown_content: str) -> Dict[str, Any]:
        """
        ä» Markdown æŠ¥å‘Šä¸­æå–ç»“æ„åŒ–æ•°æ®

        æå–ï¼š
        - highlightsï¼ˆä»"ä»Šæ—¥é‡ç‚¹"æˆ–"é‡ç‚¹"ç« èŠ‚ï¼‰
        - todosï¼ˆä»"å¾…åŠ"æˆ–"ä»»åŠ¡"ç« èŠ‚ï¼‰
        - å®Œæ•´ markdownï¼ˆç”¨äºæ˜¾ç¤ºï¼‰

        è¿”å›ä¸æ•°æ®åº“å­˜å‚¨å…¼å®¹çš„ç»“æ„

        Args:
            markdown_content: Markdown æ ¼å¼çš„æŠ¥å‘Šå†…å®¹

        Returns:
            åŒ…å«ç»“æ„åŒ–æ•°æ®çš„å­—å…¸
        """
        result = {
            'format': 'markdown',
            'full_content': markdown_content,
            'highlights': [],
            'todos': [],
            'sections': {}
        }

        try:
            # ä½¿ç”¨æ­£åˆ™æå–ç« èŠ‚ï¼š## ç« èŠ‚å\nå†…å®¹...
            # æ³¨æ„ï¼šæŠ¥å‘Šå†…å®¹é€šå¸¸ä»¥ "## ..." å¼€å¤´ï¼ŒåŸå®ç°ä½¿ç”¨ "\n##" ä¼šæ¼æ‰é¦–ä¸ªç« èŠ‚ã€‚
            content_for_split = markdown_content
            if not content_for_split.startswith("\n"):
                content_for_split = "\n" + content_for_split
            sections = re.split(r'\n##\s+', content_for_split)

            for section in sections:
                if not section.strip():
                    continue

                # æå–ç« èŠ‚æ ‡é¢˜å’Œå†…å®¹
                lines = section.split('\n', 1)
                if len(lines) < 2:
                    continue

                title = lines[0].strip()
                content = lines[1].strip()

                # å­˜å‚¨ç« èŠ‚
                result['sections'][title] = content

                # è¯†åˆ«"é‡ç‚¹"ã€"å¾…åŠ"ç­‰å…³é”®è¯
                title_lower = title.lower()

                if any(keyword in title_lower for keyword in ['é‡ç‚¹', 'highlight', 'å‘ç°']):
                    # æå–åˆ—è¡¨é¡¹
                    # æ”¯æŒ "- xxx" / "* xxx" / "1. xxx" ä¸‰ç§å¸¸è§æ ¼å¼
                    items = []
                    for m in re.finditer(r'^(?:[-*]|\d+\.)\s+(.+)$', content, re.MULTILINE):
                        items.append(m.group(1).strip())
                    result['highlights'].extend(items[:7])

                elif any(keyword in title_lower for keyword in ['å¾…åŠ', 'todo', 'ä»»åŠ¡', 'task']):
                    # æå–åˆ—è¡¨é¡¹
                    # æ”¯æŒ "- [ ] xxx" / "- [x] xxx" / "- xxx" / "1. xxx"
                    items = []
                    for m in re.finditer(r'^(?:[-*]|\d+\.)\s+(?:\[[ xX]\]\s*)?(.+)$', content, re.MULTILINE):
                        items.append(m.group(1).strip())
                    result['todos'].extend(items)

            # å¦‚æœæ²¡æœ‰æå–åˆ° highlightsï¼Œå°è¯•ä»ç¬¬ä¸€æ®µæå–
            if not result['highlights'] and markdown_content:
                first_lines = markdown_content.split('\n\n')[0]
                if first_lines:
                    result['highlights'].append(first_lines[:200])

        except Exception as e:
            logger.warning(f"è§£æ Markdown æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
            # å¤±è´¥æ—¶è‡³å°‘ä¿ç•™å®Œæ•´å†…å®¹
            result['highlights'] = ['æŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹å®Œæ•´å†…å®¹']

        return result

    @staticmethod
    def _fix_report_structure(summary: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¿®å¤ä¸å®Œæ•´çš„æŠ¥å‘Šç»“æ„

        Args:
            summary: åŸå§‹æŠ¥å‘Š

        Returns:
            ä¿®å¤åçš„æŠ¥å‘Š
        """
        # Markdown æ ¼å¼
        if summary.get('format') == 'markdown':
            if 'full_content' not in summary:
                summary['full_content'] = 'æŠ¥å‘Šç”Ÿæˆä¸­å‡ºç°å¼‚å¸¸'
            if 'highlights' not in summary:
                summary['highlights'] = ['æŠ¥å‘Šç”Ÿæˆä¸­å‡ºç°å¼‚å¸¸']
            if 'todos' not in summary:
                summary['todos'] = []
            return summary

        # JSON æ ¼å¼ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
        if 'highlights' not in summary:
            summary['highlights'] = ['æŠ¥å‘Šç”Ÿæˆä¸­å‡ºç°å¼‚å¸¸']

        if 'todos' not in summary:
            summary['todos'] = []

        if 'categories' not in summary:
            summary['categories'] = {
                'action_required': [],
                'important': [],
                'billing': [],
                'social': [],
                'other': []
            }

        return summary
